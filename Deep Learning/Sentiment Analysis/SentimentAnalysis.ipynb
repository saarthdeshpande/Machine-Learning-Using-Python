{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomUniform, glorot_uniform\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM, Dropout, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse .xml Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse .xml data to a .csv format\n",
    "\n",
    "path_train = r\"ABSA16_Laptops_Train_English_SB2.xml\"\n",
    "path_test = r\"EN_LAPT_SB2_TEST.xml\"\n",
    "\n",
    "def get_list(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_list = []\n",
    "    opinion_list = []\n",
    "    for review in root.findall(\"Review\"):\n",
    "        text_string = \"\"\n",
    "        opinion_inner_list = []\n",
    "        for sent in review.findall(\"./sentences/sentence\"):\n",
    "            text_string=text_string + \" \" + sent.find(\"text\").text\n",
    "        text_list.append(text_string)\n",
    "        for opinion in review.findall(\"./Opinions/Opinion\"):\n",
    "            opinion_dict = {\n",
    "                    opinion.get(\"category\").replace(\"#\", \"_\"): opinion.get(\"polarity\")\n",
    "            }\n",
    "            opinion_inner_list.append(opinion_dict)\n",
    "        opinion_list.append(opinion_inner_list)\n",
    "    return text_list, opinion_list\n",
    "\n",
    "train_text_list, train_opinion_list = get_list(path_train)\n",
    "test_text_list, test_opinion_list = get_list(path_test)\n",
    "\n",
    "# start fileout\n",
    "csvfile = 'ABSA16_Laptops_TrainTest_English_SB2.csv'\n",
    "fileout = open(csvfile,'wt')\n",
    "csvwrite = csv.writer(fileout)\n",
    "csvwrite.writerow(('Id', 'Text', 'Sentiment', 'Aspect'))\n",
    "\n",
    "def parse_lists(text_list, op_list, txtadjust, aspcount):\n",
    "    txtcount = 0\n",
    "    for text in text_list:\n",
    "        opinions = op_list[txtcount]\n",
    "        printcount = txtcount + txtadjust  # for the test set\n",
    "        txtcount += 1\n",
    "        for op in opinions:\n",
    "            for aspect in op:\n",
    "                senti = op[aspect]\n",
    "                csvwrite.writerow((printcount, text, senti, aspect))\n",
    "                aspcount += 1\n",
    "    return(printcount, aspcount)\n",
    "\n",
    "(traintotaltxt, traintotalasp) = parse_lists(train_text_list, train_opinion_list, 0, 0)\n",
    "(testtotaltxt, testtotalasp) = parse_lists(test_text_list, test_opinion_list, traintotaltxt, traintotalasp)\n",
    "\n",
    "print('%d texts processed including %d aspects; saved to %s' % (testtotaltxt, testtotalasp, csvfile))\n",
    "fileout.close()\n",
    "\n",
    "xtree = et.parse(\"ABSA16_Laptops_Train_English_SB2.xml\")\n",
    "xroot = xtree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data, adjust column names, clean if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('dataset.csv')\n",
    "trainData.columns = ['Id', 'Text', 'Sentiment', 'Aspect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to handle similar aspects (e.g.: LAPTOP_GENERAL, LAPTOP_USABILITY)\n",
    "def betterLabels(entry):\n",
    "    if entry.startswith('COMPANY'):\n",
    "        return 'General'\n",
    "    elif entry.startswith('LAPTOP'):\n",
    "        return 'General'\n",
    "    elif entry.startswith('MOUSE'):\n",
    "        return 'Mouse'\n",
    "    elif entry.startswith('KEYBOARD'):\n",
    "        return 'Keyboard'\n",
    "    elif entry.startswith('OS'):\n",
    "        return 'OS'\n",
    "    elif entry.startswith('DISPLAY'):\n",
    "        return 'Display'\n",
    "    elif entry.startswith('SOFTWARE'):\n",
    "        return 'Software'\n",
    "    elif entry.startswith('BATTERY'):\n",
    "        return 'Battery'\n",
    "    elif entry.startswith('HARD'):\n",
    "        return 'Hardware'\n",
    "    elif entry.startswith('GRAPHICS'):\n",
    "        return 'Graphics'\n",
    "    elif entry.startswith('POWER_SUPPLY'):\n",
    "        return 'Power_Supply'\n",
    "    elif entry.startswith('PORTS'):\n",
    "        return 'Ports'\n",
    "    elif entry.startswith('CPU'):\n",
    "        return 'CPU'\n",
    "    elif entry.startswith('MULTIMEDIA'):\n",
    "        return 'Multimedia'\n",
    "    elif entry.endswith('PRICE'):\n",
    "        return 'Price'\n",
    "    elif entry.startswith('SUPPORT'):\n",
    "        return 'Support'\n",
    "    elif entry.startswith('MOTHERBOARD'):\n",
    "        return 'Hardware'\n",
    "    elif entry.startswith('OPTICAL'):\n",
    "        return 'Hardware'\n",
    "    elif entry.startswith('FANS'):\n",
    "        return 'Hardware'\n",
    "    elif entry.endswith('GENERAL'):\n",
    "        return 'General'\n",
    "    elif entry.endswith('QUALITY'):\n",
    "        return 'General'\n",
    "    return entry\n",
    "\n",
    "trainData['Aspect'] = trainData['Aspect'].apply(betterLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.drop_duplicates(inplace=True)\n",
    "trainData2 = trainData[trainData['Aspect'] != 'General']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>aspect_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_GENERAL</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_OPERATION_PERFORMANCE</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>HARD_DISC_DESIGN_FEATURES</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_QUALITY</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>DISPLAY_QUALITY</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_DESIGN_FEATURES</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>KEYBOARD_GENERAL</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_OPERATION_PERFORMANCE</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_GENERAL</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LAPTOP_USABILITY</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text Sentiment  \\\n",
       "0   0   most everything is fine with this machine: sp...  positive   \n",
       "1   0   most everything is fine with this machine: sp...  positive   \n",
       "2   0   most everything is fine with this machine: sp...  positive   \n",
       "3   0   most everything is fine with this machine: sp...  positive   \n",
       "4   0   most everything is fine with this machine: sp...  negative   \n",
       "5   1   i love the size, keyboard, the functions. i d...  positive   \n",
       "6   1   i love the size, keyboard, the functions. i d...  positive   \n",
       "7   1   i love the size, keyboard, the functions. i d...  positive   \n",
       "8   1   i love the size, keyboard, the functions. i d...  positive   \n",
       "9   1   i love the size, keyboard, the functions. i d...  positive   \n",
       "\n",
       "                         Aspect  \\\n",
       "0                LAPTOP_GENERAL   \n",
       "1  LAPTOP_OPERATION_PERFORMANCE   \n",
       "2     HARD_DISC_DESIGN_FEATURES   \n",
       "3                LAPTOP_QUALITY   \n",
       "4               DISPLAY_QUALITY   \n",
       "5        LAPTOP_DESIGN_FEATURES   \n",
       "6              KEYBOARD_GENERAL   \n",
       "7  LAPTOP_OPERATION_PERFORMANCE   \n",
       "8                LAPTOP_GENERAL   \n",
       "9              LAPTOP_USABILITY   \n",
       "\n",
       "                                        aspect_terms  \n",
       "0  machine speed capacity thing resolution screen...  \n",
       "1  machine speed capacity thing resolution screen...  \n",
       "2  machine speed capacity thing resolution screen...  \n",
       "3  machine speed capacity thing resolution screen...  \n",
       "4  machine speed capacity thing resolution screen...  \n",
       "5  size keyboard functions complaint quality pric...  \n",
       "6  size keyboard functions complaint quality pric...  \n",
       "7  size keyboard functions complaint quality pric...  \n",
       "8  size keyboard functions complaint quality pric...  \n",
       "9  size keyboard functions complaint quality pric...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all aspect terms per sentence together\n",
    "\n",
    "trainData.Text = trainData.Text.str.lower()\n",
    "aspect_terms = []\n",
    "for review in nlp.pipe(trainData.Text):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    aspect_terms.append(' '.join(chunks))\n",
    "trainData['aspect_terms'] = aspect_terms\n",
    "trainData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to numpy arrays\n",
    "\n",
    "vocab_size = 6000 # We set a maximum size for the vocabulary\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(trainData2.Text)\n",
    "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(trainData2.aspect_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspects to numpy arrays\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_category = label_encoder.fit_transform(trainData2.Aspect)\n",
    "dummy_category = to_categorical(integer_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model for aspect detection\n",
    "\n",
    "aspectSimple = Sequential()\n",
    "aspectSimple.add(Dense(512, input_shape=(6000,), activation='relu'))\n",
    "aspectSimple.add(Dense(256, activation='relu'))\n",
    "aspectSimple.add(Dense(128, activation='relu'))\n",
    "aspectSimple.add(Dense(128, activation='relu'))\n",
    "aspectSimple.add(Dense(15, activation='softmax'))\n",
    "aspectSimple.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex model for aspect detection, not used for inference\n",
    "\n",
    "aspect_categories_model = Sequential()\n",
    "\n",
    "e_init = RandomUniform(-0.01, 0.01, seed=1)\n",
    "init = glorot_uniform(seed=1)\n",
    "\n",
    "aspect_categories_model.add(Embedding(input_dim=6000,\n",
    "  output_dim=32, embeddings_initializer=e_init,\n",
    "  mask_zero=True))\n",
    "aspect_categories_model.add(LSTM(units=100, kernel_initializer=init,\n",
    "  dropout=0.2, recurrent_dropout=0.2))  # 100 memory\n",
    "aspect_categories_model.add(Dense(units=16, kernel_initializer=init,\n",
    "  activation='sigmoid'))\n",
    "aspect_categories_model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "577/577 [==============================] - 0s 443us/step - loss: 2.6341 - accuracy: 0.1386\n",
      "Epoch 2/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 2.3944 - accuracy: 0.1958\n",
      "Epoch 3/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 2.1065 - accuracy: 0.3362\n",
      "Epoch 4/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 1.8526 - accuracy: 0.4090\n",
      "Epoch 5/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 1.6192 - accuracy: 0.4263\n",
      "Epoch 6/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 1.5020 - accuracy: 0.4315\n",
      "Epoch 7/100\n",
      "577/577 [==============================] - 0s 250us/step - loss: 1.3885 - accuracy: 0.4385\n",
      "Epoch 8/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 1.3470 - accuracy: 0.4385\n",
      "Epoch 9/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 1.3408 - accuracy: 0.4437\n",
      "Epoch 10/100\n",
      "577/577 [==============================] - 0s 235us/step - loss: 1.3194 - accuracy: 0.4662\n",
      "Epoch 11/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 1.2046 - accuracy: 0.4437\n",
      "Epoch 12/100\n",
      "577/577 [==============================] - 0s 243us/step - loss: 1.2368 - accuracy: 0.4367\n",
      "Epoch 13/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 1.2195 - accuracy: 0.4367\n",
      "Epoch 14/100\n",
      "577/577 [==============================] - 0s 235us/step - loss: 1.1638 - accuracy: 0.4471\n",
      "Epoch 15/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 1.1531 - accuracy: 0.4350\n",
      "Epoch 16/100\n",
      "577/577 [==============================] - 0s 238us/step - loss: 1.1167 - accuracy: 0.4385\n",
      "Epoch 17/100\n",
      "577/577 [==============================] - 0s 243us/step - loss: 1.1137 - accuracy: 0.4402\n",
      "Epoch 18/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 1.1153 - accuracy: 0.4506\n",
      "Epoch 19/100\n",
      "577/577 [==============================] - 0s 243us/step - loss: 1.1572 - accuracy: 0.4714\n",
      "Epoch 20/100\n",
      "577/577 [==============================] - 0s 252us/step - loss: 1.1338 - accuracy: 0.4697\n",
      "Epoch 21/100\n",
      "577/577 [==============================] - 0s 250us/step - loss: 1.0783 - accuracy: 0.4385\n",
      "Epoch 22/100\n",
      "577/577 [==============================] - 0s 251us/step - loss: 1.0288 - accuracy: 0.4506\n",
      "Epoch 23/100\n",
      "577/577 [==============================] - 0s 236us/step - loss: 1.1427 - accuracy: 0.4315\n",
      "Epoch 24/100\n",
      "577/577 [==============================] - 0s 240us/step - loss: 1.0894 - accuracy: 0.4558\n",
      "Epoch 25/100\n",
      "577/577 [==============================] - 0s 243us/step - loss: 1.0620 - accuracy: 0.4610\n",
      "Epoch 26/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 1.0821 - accuracy: 0.4506\n",
      "Epoch 27/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 1.0588 - accuracy: 0.4437\n",
      "Epoch 28/100\n",
      "577/577 [==============================] - 0s 251us/step - loss: 1.0198 - accuracy: 0.4610\n",
      "Epoch 29/100\n",
      "577/577 [==============================] - 0s 248us/step - loss: 0.9888 - accuracy: 0.4489\n",
      "Epoch 30/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 1.0467 - accuracy: 0.4402\n",
      "Epoch 31/100\n",
      "577/577 [==============================] - 0s 240us/step - loss: 1.0724 - accuracy: 0.4593\n",
      "Epoch 32/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 1.0421 - accuracy: 0.4523\n",
      "Epoch 33/100\n",
      "577/577 [==============================] - 0s 256us/step - loss: 1.0237 - accuracy: 0.4749\n",
      "Epoch 34/100\n",
      "577/577 [==============================] - 0s 250us/step - loss: 1.0615 - accuracy: 0.4697\n",
      "Epoch 35/100\n",
      "577/577 [==============================] - 0s 237us/step - loss: 1.0457 - accuracy: 0.4437\n",
      "Epoch 36/100\n",
      "577/577 [==============================] - 0s 245us/step - loss: 1.0242 - accuracy: 0.4541\n",
      "Epoch 37/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 1.0441 - accuracy: 0.4454\n",
      "Epoch 38/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 1.0561 - accuracy: 0.4454\n",
      "Epoch 39/100\n",
      "577/577 [==============================] - 0s 233us/step - loss: 1.0292 - accuracy: 0.4905\n",
      "Epoch 40/100\n",
      "577/577 [==============================] - 0s 256us/step - loss: 1.0241 - accuracy: 0.4731\n",
      "Epoch 41/100\n",
      "577/577 [==============================] - 0s 261us/step - loss: 1.0391 - accuracy: 0.4714\n",
      "Epoch 42/100\n",
      "577/577 [==============================] - 0s 253us/step - loss: 1.0528 - accuracy: 0.4662\n",
      "Epoch 43/100\n",
      "577/577 [==============================] - 0s 245us/step - loss: 0.9905 - accuracy: 0.4541\n",
      "Epoch 44/100\n",
      "577/577 [==============================] - 0s 239us/step - loss: 0.9760 - accuracy: 0.4766\n",
      "Epoch 45/100\n",
      "577/577 [==============================] - 0s 245us/step - loss: 0.9978 - accuracy: 0.4627\n",
      "Epoch 46/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 0.9865 - accuracy: 0.4627\n",
      "Epoch 47/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 0.9651 - accuracy: 0.4697\n",
      "Epoch 48/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 0.9833 - accuracy: 0.4610\n",
      "Epoch 49/100\n",
      "577/577 [==============================] - 0s 247us/step - loss: 0.9545 - accuracy: 0.4610\n",
      "Epoch 50/100\n",
      "577/577 [==============================] - 0s 239us/step - loss: 0.9465 - accuracy: 0.4471\n",
      "Epoch 51/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 0.9736 - accuracy: 0.4714\n",
      "Epoch 52/100\n",
      "577/577 [==============================] - 0s 265us/step - loss: 1.0039 - accuracy: 0.4801\n",
      "Epoch 53/100\n",
      "577/577 [==============================] - 0s 270us/step - loss: 1.0072 - accuracy: 0.4783\n",
      "Epoch 54/100\n",
      "577/577 [==============================] - 0s 258us/step - loss: 1.0387 - accuracy: 0.4697\n",
      "Epoch 55/100\n",
      "577/577 [==============================] - 0s 277us/step - loss: 1.0148 - accuracy: 0.4489\n",
      "Epoch 56/100\n",
      "577/577 [==============================] - 0s 258us/step - loss: 1.0261 - accuracy: 0.4749\n",
      "Epoch 57/100\n",
      "577/577 [==============================] - 0s 239us/step - loss: 0.9940 - accuracy: 0.4645\n",
      "Epoch 58/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 0.9812 - accuracy: 0.4679\n",
      "Epoch 59/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 0.9773 - accuracy: 0.4731\n",
      "Epoch 60/100\n",
      "577/577 [==============================] - 0s 247us/step - loss: 1.0024 - accuracy: 0.4766\n",
      "Epoch 61/100\n",
      "577/577 [==============================] - 0s 248us/step - loss: 0.9772 - accuracy: 0.4541\n",
      "Epoch 62/100\n",
      "577/577 [==============================] - 0s 248us/step - loss: 0.9629 - accuracy: 0.4610\n",
      "Epoch 63/100\n",
      "577/577 [==============================] - 0s 240us/step - loss: 0.9537 - accuracy: 0.4541\n",
      "Epoch 64/100\n",
      "577/577 [==============================] - 0s 261us/step - loss: 0.9830 - accuracy: 0.4870\n",
      "Epoch 65/100\n",
      "577/577 [==============================] - 0s 257us/step - loss: 1.0090 - accuracy: 0.4697\n",
      "Epoch 66/100\n",
      "577/577 [==============================] - 0s 251us/step - loss: 1.0011 - accuracy: 0.4801\n",
      "Epoch 67/100\n",
      "577/577 [==============================] - 0s 248us/step - loss: 0.9615 - accuracy: 0.4593\n",
      "Epoch 68/100\n",
      "577/577 [==============================] - 0s 259us/step - loss: 0.9446 - accuracy: 0.4749\n",
      "Epoch 69/100\n",
      "577/577 [==============================] - 0s 264us/step - loss: 1.0112 - accuracy: 0.4402\n",
      "Epoch 70/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 0.9927 - accuracy: 0.4835\n",
      "Epoch 71/100\n",
      "577/577 [==============================] - 0s 253us/step - loss: 0.9768 - accuracy: 0.4662\n",
      "Epoch 72/100\n",
      "577/577 [==============================] - 0s 255us/step - loss: 0.9799 - accuracy: 0.4558\n",
      "Epoch 73/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 0.9665 - accuracy: 0.4783\n",
      "Epoch 74/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9696 - accuracy: 0.4489\n",
      "Epoch 75/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9527 - accuracy: 0.4645\n",
      "Epoch 76/100\n",
      "577/577 [==============================] - 0s 250us/step - loss: 0.9661 - accuracy: 0.4697\n",
      "Epoch 77/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 0.9631 - accuracy: 0.4783\n",
      "Epoch 78/100\n",
      "577/577 [==============================] - 0s 245us/step - loss: 0.9414 - accuracy: 0.4333\n",
      "Epoch 79/100\n",
      "577/577 [==============================] - 0s 243us/step - loss: 0.9882 - accuracy: 0.4627\n",
      "Epoch 80/100\n",
      "577/577 [==============================] - 0s 250us/step - loss: 0.9694 - accuracy: 0.4593\n",
      "Epoch 81/100\n",
      "577/577 [==============================] - 0s 238us/step - loss: 0.9860 - accuracy: 0.4749\n",
      "Epoch 82/100\n",
      "577/577 [==============================] - 0s 235us/step - loss: 0.9520 - accuracy: 0.4575\n",
      "Epoch 83/100\n",
      "577/577 [==============================] - 0s 237us/step - loss: 0.9627 - accuracy: 0.4593\n",
      "Epoch 84/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9839 - accuracy: 0.4645\n",
      "Epoch 85/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9473 - accuracy: 0.45750s - loss: 0.8524 - accuracy: 0.\n",
      "Epoch 86/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 1.0005 - accuracy: 0.4593\n",
      "Epoch 87/100\n",
      "577/577 [==============================] - 0s 247us/step - loss: 1.0383 - accuracy: 0.4766\n",
      "Epoch 88/100\n",
      "577/577 [==============================] - 0s 249us/step - loss: 1.0139 - accuracy: 0.4853\n",
      "Epoch 89/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 0.9794 - accuracy: 0.4627\n",
      "Epoch 90/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 1.0210 - accuracy: 0.4662\n",
      "Epoch 91/100\n",
      "577/577 [==============================] - 0s 241us/step - loss: 0.9878 - accuracy: 0.4558\n",
      "Epoch 92/100\n",
      "577/577 [==============================] - 0s 238us/step - loss: 0.9874 - accuracy: 0.4731\n",
      "Epoch 93/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 0.9483 - accuracy: 0.4749\n",
      "Epoch 94/100\n",
      "577/577 [==============================] - 0s 256us/step - loss: 0.9273 - accuracy: 0.4731\n",
      "Epoch 95/100\n",
      "577/577 [==============================] - 0s 244us/step - loss: 0.9544 - accuracy: 0.4697\n",
      "Epoch 96/100\n",
      "577/577 [==============================] - 0s 242us/step - loss: 0.9582 - accuracy: 0.4835\n",
      "Epoch 97/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9722 - accuracy: 0.4731\n",
      "Epoch 98/100\n",
      "577/577 [==============================] - 0s 246us/step - loss: 0.9300 - accuracy: 0.4645\n",
      "Epoch 99/100\n",
      "577/577 [==============================] - 0s 252us/step - loss: 0.9234 - accuracy: 0.4558\n",
      "Epoch 100/100\n",
      "577/577 [==============================] - 0s 239us/step - loss: 0.9245 - accuracy: 0.4783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f99b0372a10>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspectSimple.fit(aspect_tokenized, dummy_category, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1126/1126 [==============================] - 339s 301ms/step - loss: 0.4197 - acc: 0.9345\n",
      "Epoch 2/5\n",
      "1126/1126 [==============================] - 356s 316ms/step - loss: 0.2076 - acc: 0.9380\n",
      "Epoch 3/5\n",
      "1126/1126 [==============================] - 364s 323ms/step - loss: 0.1934 - acc: 0.9391\n",
      "Epoch 4/5\n",
      "1126/1126 [==============================] - 364s 324ms/step - loss: 0.1883 - acc: 0.9396\n",
      "Epoch 5/5\n",
      "1126/1126 [==============================] - 368s 327ms/step - loss: 0.1851 - acc: 0.9399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9aac18aad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aspect_categories_model.fit(aspect_tokenized, dummy_category, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>sentiment_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>General</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "      <td>fine understand high high available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "      <td>fine understand high high available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>most everything is fine with this machine: sp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Display</td>\n",
       "      <td>machine speed capacity thing resolution screen...</td>\n",
       "      <td>fine understand high high available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>General</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "      <td>love easy use good good recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>i love the size, keyboard, the functions. i d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>size keyboard functions complaint quality pric...</td>\n",
       "      <td>love easy use good good recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>i love this product because it is toshiba and...</td>\n",
       "      <td>positive</td>\n",
       "      <td>General</td>\n",
       "      <td>product camera connect downside product</td>\n",
       "      <td>love buy install easy use compatible recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>i love this product because it is toshiba and...</td>\n",
       "      <td>negative</td>\n",
       "      <td>General</td>\n",
       "      <td>product camera connect downside product</td>\n",
       "      <td>love buy install easy use compatible recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>i bought this laptop was the worst laptop i'v...</td>\n",
       "      <td>negative</td>\n",
       "      <td>General</td>\n",
       "      <td>laptop laptop alot money product nightmare com...</td>\n",
       "      <td>buy bad buy spend have deal bad send fix perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>i bought this laptop was the worst laptop i'v...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Support</td>\n",
       "      <td>laptop laptop alot money product nightmare com...</td>\n",
       "      <td>buy bad buy spend have deal bad send fix perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>so far, a great product. high price tag, howe...</td>\n",
       "      <td>positive</td>\n",
       "      <td>General</td>\n",
       "      <td>product tag</td>\n",
       "      <td>great high try learn use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                               Text Sentiment    Aspect  \\\n",
       "0    0   most everything is fine with this machine: sp...  positive   General   \n",
       "2    0   most everything is fine with this machine: sp...  positive  Hardware   \n",
       "4    0   most everything is fine with this machine: sp...  negative   Display   \n",
       "5    1   i love the size, keyboard, the functions. i d...  positive   General   \n",
       "6    1   i love the size, keyboard, the functions. i d...  positive  Keyboard   \n",
       "12   2   i love this product because it is toshiba and...  positive   General   \n",
       "15   2   i love this product because it is toshiba and...  negative   General   \n",
       "16   3   i bought this laptop was the worst laptop i'v...  negative   General   \n",
       "18   3   i bought this laptop was the worst laptop i'v...  negative   Support   \n",
       "20   4   so far, a great product. high price tag, howe...  positive   General   \n",
       "\n",
       "                                         aspect_terms  \\\n",
       "0   machine speed capacity thing resolution screen...   \n",
       "2   machine speed capacity thing resolution screen...   \n",
       "4   machine speed capacity thing resolution screen...   \n",
       "5   size keyboard functions complaint quality pric...   \n",
       "6   size keyboard functions complaint quality pric...   \n",
       "12            product camera connect downside product   \n",
       "15            product camera connect downside product   \n",
       "16  laptop laptop alot money product nightmare com...   \n",
       "18  laptop laptop alot money product nightmare com...   \n",
       "20                                        product tag   \n",
       "\n",
       "                                      sentiment_terms  \n",
       "0                 fine understand high high available  \n",
       "2                 fine understand high high available  \n",
       "4                 fine understand high high available  \n",
       "5                   love easy use good good recommend  \n",
       "6                   love easy use good good recommend  \n",
       "12     love buy install easy use compatible recommend  \n",
       "15     love buy install easy use compatible recommend  \n",
       "16  buy bad buy spend have deal bad send fix perfe...  \n",
       "18  buy bad buy spend have deal bad send fix perfe...  \n",
       "20                           great high try learn use  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all sentiment terms per sentence together\n",
    "\n",
    "sentiment_terms = []\n",
    "for review in nlp.pipe(trainData['Text']):\n",
    "        if review.is_parsed:\n",
    "            sentiment_terms.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n",
    "        else:\n",
    "            sentiment_terms.append('')  \n",
    "trainData['sentiment_terms'] = sentiment_terms\n",
    "trainData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(trainData.sentiment_terms))\n",
    "\n",
    "label_encoder_2 = LabelEncoder()\n",
    "integer_sentiment = label_encoder_2.fit_transform(trainData.Sentiment)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # complex model for sentiment detection, not used for inference\n",
    "\n",
    "sentimentComplex = Sequential()\n",
    "e_init = RandomUniform(-0.01, 0.01, seed=1)\n",
    "init = glorot_uniform(seed=1)\n",
    "\n",
    "sentimentComplex.add(Embedding(input_dim=6000,\n",
    "  output_dim=64, embeddings_initializer=e_init,\n",
    "  mask_zero=True))\n",
    "sentimentComplex.add(LSTM(units=100, kernel_initializer=init,\n",
    "  dropout=0.2, recurrent_dropout=0.2))  # 100 memory\n",
    "sentimentComplex.add(Dense(units=4, kernel_initializer=init,\n",
    "  activation='sigmoid'))\n",
    "sentimentComplex.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model for sentiment detection\n",
    "\n",
    "sentiment_model = Sequential()\n",
    "sentiment_model.add(Dense(256, input_shape=(6000,), activation='relu'))\n",
    "sentiment_model.add(Dense(128, activation='relu'))\n",
    "sentiment_model.add(Dense(128, activation='relu'))\n",
    "sentiment_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "sentiment_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1126/1126 [==============================] - 0s 267us/step - loss: 1.0801 - accuracy: 0.6163\n",
      "Epoch 2/5\n",
      "1126/1126 [==============================] - 0s 160us/step - loss: 0.8060 - accuracy: 0.7105\n",
      "Epoch 3/5\n",
      "1126/1126 [==============================] - 0s 156us/step - loss: 0.7336 - accuracy: 0.7318\n",
      "Epoch 4/5\n",
      "1126/1126 [==============================] - 0s 158us/step - loss: 0.6805 - accuracy: 0.7336\n",
      "Epoch 5/5\n",
      "1126/1126 [==============================] - 0s 154us/step - loss: 0.6458 - accuracy: 0.7256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f99d46e74d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(sentiment_tokenized, dummy_sentiment, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1 expresses a negative opinion about the Mouse\n",
      "Sentence #2 expresses a positive opinion about the Hardware\n",
      "Sentence #3 expresses a negative opinion about the Display\n",
      "Sentence #4 expresses a negative opinion about the Battery\n",
      "Sentence #5 expresses a positive opinion about the Display\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'worst mouse ever',\n",
    "    'hardware is fantastic',\n",
    "    'display is a disappointment',\n",
    "    'battery is crap',\n",
    "    'display is lovely'    \n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    \n",
    "    # aspect detection\n",
    "    \n",
    "    chunks = [(chunk.root.text) for chunk in nlp(sentence).noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    new_review_aspect_terms = ' '.join(chunks)\n",
    "    new_review_aspect_tokenized = tokenizer.texts_to_matrix([new_review_aspect_terms])\n",
    "\n",
    "    aspect = label_encoder.inverse_transform(aspectSimple.predict_classes(new_review_aspect_tokenized))\n",
    "    \n",
    "    # sentiment detection\n",
    "    \n",
    "    chunks = [(chunk.root.text) for chunk in nlp(sentence).noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    new_review_aspect_terms = ' '.join(chunks)\n",
    "    new_review_aspect_tokenized = tokenizer.texts_to_matrix([new_review_aspect_terms])\n",
    "    sentiment = label_encoder_2.inverse_transform(sentiment_model.predict_classes(new_review_aspect_tokenized))\n",
    "    print('Sentence #' + str(test_sentences.index(sentence) + 1) + \" expresses a \" + sentiment[0] + ' opinion about the ' + aspect[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
